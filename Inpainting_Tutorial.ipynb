{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image_Super Resolve_Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rudrabha/SS2021-19-08-2021/blob/main/Inpainting_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA26ItOODFRL"
      },
      "source": [
        "**Import Headers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UON2uQnNAuDz",
        "outputId": "52dd54e0-28ec-44bb-ee0a-e2300ef7cac7"
      },
      "source": [
        "import os\n",
        "!pip install wget\n",
        "import wget\n",
        "import shutil\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iqW-Yjhk5OL"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch \n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch import optim"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkrsI3rEl4A7",
        "outputId": "4b7176af-ed6d-4d59-cf59-0e1e96d5e648"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "print('use_cuda: {}'.format(use_cuda))\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(\"Device to be used : \",device)\n",
        "!nvidia-smi"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "use_cuda: True\n",
            "Device to be used :  cuda\n",
            "Wed Aug 18 17:25:08 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.57.02    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8     9W /  70W |      3MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4D25A8gDK4r"
      },
      "source": [
        "**Setting up Data Path**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qM68zdO2IRUH"
      },
      "source": [
        "#shutil.rmtree(\"/content/IMAGE_SUPER_RESOLVE_DATA/extracted_data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd4gLw6uDV-E"
      },
      "source": [
        "parent_folder = \"/content/IMAGE_SUPER_RESOLVE_DATA\"\n",
        "\n",
        "if os.path.isdir(parent_folder):\n",
        "    shutil.rmtree(parent_folder)\n",
        "os.mkdir(parent_folder)\n",
        "\n",
        "#Create Folder to download Raw Data\n",
        "raw_data_folder = os.path.join(parent_folder,\"raw_data\")\n",
        "extracted_data_folder = os.path.join(parent_folder,\"extracted_data\")\n",
        "\n",
        "if not os.path.isdir(raw_data_folder):\n",
        "    os.mkdir(raw_data_folder)\n",
        "\n",
        "if not os.path.isdir(extracted_data_folder):\n",
        "    os.mkdir(extracted_data_folder)\n",
        "\n",
        "image_data_folder = os.path.join(extracted_data_folder, \"images\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n-gyvN7EXMi"
      },
      "source": [
        "**Downloading Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBnxcgceFT84",
        "outputId": "2fee9f2c-8f6c-474c-e522-61df259b6e04"
      },
      "source": [
        "dataset_link = \"https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\"\n",
        "raw_data = os.path.join(raw_data_folder, \"images.tar.gz\")\n",
        "print(\"Downloading Data\")\n",
        "wget.download(dataset_link, raw_data)\n",
        "print(\"Downloading Done\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Data\n",
            "Downloading Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pndT1J33JG1H"
      },
      "source": [
        "**Extracting the Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJgY_Nd3JGCQ"
      },
      "source": [
        "shutil.unpack_archive(raw_data, extracted_data_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ys5YOmXK2ZZ"
      },
      "source": [
        "**Listing the Dataset Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9_u9N2EKuEd"
      },
      "source": [
        "def get_image_address(image_data_folder):\n",
        "    image_address_list = []\n",
        "    image_address_list = glob.glob(os.path.join(image_data_folder, \"*.jpg\"))\n",
        "    print(\"Number of Files : \", len(image_address_list))\n",
        "    for img_addr in image_address_list:\n",
        "        try :\n",
        "            img = cv2.imread(img_addr)\n",
        "            x = img.shape\n",
        "        except :\n",
        "            image_address_list.remove(img_addr)\n",
        "            os.remove(img_addr)\n",
        "        \n",
        "    print(\"Number of Files after removing : \", len(image_address_list))\n",
        "\n",
        "    return image_address_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MJylShMkZQf"
      },
      "source": [
        "**MODULE_1 : Data Loader**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLBq5_zjkd9u"
      },
      "source": [
        "class DataGenerator(Dataset):\n",
        "\t\n",
        "    def __init__(self, image_list):\n",
        "        self.files = image_list\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "        \n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "\n",
        "        #print(files[idx])\n",
        "        img = cv2.imread(self.files[idx])\n",
        "        high_res_img = cv2.resize(img,(512,512))\n",
        "        high_res_img = np.transpose(high_res_img, (2, 0, 1))\n",
        "        low_res_img = cv2.resize(img,(128,128))\n",
        "        low_res_img = cv2.resize(low_res_img, (512, 512))\n",
        "        low_res_img = np.transpose(low_res_img, (2, 0, 1))\n",
        "        return torch.FloatTensor(high_res_img/255.), torch.FloatTensor(low_res_img/255.)\n",
        "\t\t\n",
        "\t\n",
        "def load_data(image_list, batch_size=32, num_workers=10, shuffle=True):\n",
        "\n",
        "    dataset = DataGenerator(image_list)\n",
        "    data_loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=shuffle)\n",
        "\n",
        "    return data_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Tf2iVeJvUDh"
      },
      "source": [
        "**MODULE 2 : Model Creation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-E8p0uaux9w_"
      },
      "source": [
        "**Conv2D**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEYELaVgvg5g"
      },
      "source": [
        "class Conv2d(nn.Module):\n",
        "    def __init__(self, cin, cout, kernel_size, stride, padding, residual=False, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.conv_block = nn.Sequential(\n",
        "                            nn.Conv2d(cin, cout, kernel_size, stride, padding),\n",
        "                            nn.BatchNorm2d(cout)\n",
        "                            )\n",
        "        self.act = nn.ReLU()\n",
        "        self.residual = residual\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv_block(x)\n",
        "        if self.residual:\n",
        "            out += x\n",
        "        return self.act(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bCywTOIyCIo"
      },
      "source": [
        "**Conv2D-T**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8a1mNGRx3lM"
      },
      "source": [
        "class Conv2dTranspose(nn.Module):\n",
        "    def __init__(self, cin, cout, kernel_size, stride, padding, output_padding=0, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.conv_block = nn.Sequential(\n",
        "                            nn.ConvTranspose2d(cin, cout, kernel_size, stride, padding, output_padding),\n",
        "                            nn.BatchNorm2d(cout)\n",
        "                            )\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv_block(x)\n",
        "        return self.act(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eahg7WyeyHla"
      },
      "source": [
        "**Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAYmhm0RyMJy"
      },
      "source": [
        "class Image_Super_Resolve(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Image_Super_Resolve, self).__init__()\n",
        "\n",
        "        self.image_encoder = nn.Sequential(\n",
        "            Conv2d(3, 4, kernel_size=3, stride=1, padding=1),\n",
        "            \n",
        "            Conv2d(4, 8, kernel_size=3, stride=1, padding=1),\n",
        "            Conv2d(8, 8, kernel_size=3, stride=1, padding=1, residual=True),\n",
        "            Conv2d(8, 8, kernel_size=3, stride=1, padding=1, residual=True),\n",
        "            \n",
        "            Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
        "            Conv2d(16, 16, kernel_size=3, stride=1, padding=1, residual=True),\n",
        "            Conv2d(16, 16, kernel_size=3, stride=1, padding=1, residual=True),\n",
        "            \n",
        "            Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n",
        "            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),      \n",
        "            )\n",
        "        self.image_decoder = nn.Sequential(\n",
        "\n",
        "            # Conv2dTranspose(32, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "\n",
        "            # Conv2dTranspose(32, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n",
        "            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n",
        "            Conv2d(32, 3, kernel_size=3, stride=1, padding=1),\n",
        "            )\n",
        "        \n",
        "    def forward(self, face_image):\n",
        "\n",
        "        #print(\"Shape : \",face_image.shape)\n",
        "        face_embedding = self.image_encoder(face_image)\n",
        "        # print(\"Shape : \",face_embedding.shape)\n",
        "        decoded_face = self.image_decoder(face_embedding)\n",
        "        decoded_face += face_image\n",
        "\n",
        "        decoded_face = torch.sigmoid(decoded_face)\n",
        "        # print(\"Shape : \",decoded_face.shape)\n",
        "        return decoded_face\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkhAMdpk9Wmz"
      },
      "source": [
        "**Code to check the model shape**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adkY3N__yHGs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca1b9ffe-f587-49a3-9da9-844fb6c81778"
      },
      "source": [
        "model = Image_Super_Resolve()\n",
        "data = torch.rand(2, 3, 512, 512)\n",
        "print(data.shape)\n",
        "decoded_data = model.forward(data)\n",
        "print (decoded_data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 3, 512, 512])\n",
            "torch.Size([2, 3, 512, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOBpnH7VnA4f"
      },
      "source": [
        "class PSNR:\n",
        "    \"\"\"Peak Signal to Noise Ratio\n",
        "    img1 and img2 have range [0, 255]\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.name = \"PSNR\"\n",
        "\n",
        "    @staticmethod\n",
        "    def __call__(img1, img2):\n",
        "        mse = torch.mean((img1 - img2) ** 2)\n",
        "        return 20 * torch.log10(255.0 / torch.sqrt(mse))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wg7mYyzll_A"
      },
      "source": [
        "**MODULE 3 : Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sncpj_TLGqAO"
      },
      "source": [
        "**TRAIN EPOCH**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFbFHbMuBz6S"
      },
      "source": [
        "def train_epoch(train_loader, model, optimizer, epoch):\n",
        "\n",
        "    progress_bar = tqdm(enumerate(train_loader))\n",
        "    total_loss = 0.0\n",
        "    for step, (high_res_img, low_res_img) in progress_bar:\n",
        "\n",
        "        try :\n",
        "            if high_res_img is None and low_res_img is None:\n",
        "                continue\n",
        "            model.train()\n",
        "            high_res_img = high_res_img.to(device)\n",
        "            low_res_img = low_res_img.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            pred_img = model.forward(low_res_img)\n",
        "\n",
        "            mse = nn.MSELoss()\n",
        "            psnr = PSNR()\n",
        "\n",
        "            mse_loss = mse(pred_img, high_res_img)\n",
        "            psnr_loss = psnr(pred_img*255.0, high_res_img*255.0)\n",
        "\n",
        "            loss = mse_loss\n",
        "\n",
        "            # print(loss)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            progress_bar.set_description(\n",
        "            \"Epoch : {} Training Loss : {} \".format(epoch, loss))\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    return model, optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MI5PZkyGx5J"
      },
      "source": [
        "**Validation Loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Im6okWc4BvJn"
      },
      "source": [
        "def val_epoch(val_loader, model, optimizer, epoch):\n",
        "\n",
        "    progress_bar = tqdm(enumerate(val_loader))\n",
        "    total_loss = 0.0\n",
        "    for step, (high_res_img, low_res_img) in progress_bar:\n",
        "\n",
        "        try :\n",
        "            if high_res_img is None and low_res_img is None:\n",
        "                continue\n",
        "\n",
        "            high_res_img = high_res_img.to(device)\n",
        "            low_res_img = low_res_img.to(device)\n",
        "\n",
        "            mse = nn.MSELoss()\n",
        "            psnr = PSNR()\n",
        "\n",
        "            model.eval()\n",
        "            pred_img = model.forward(low_res_img)\n",
        "\n",
        "            mse_loss = mse(pred_img, high_res_img)\n",
        "            psnr_loss = psnr(pred_img*255.0, high_res_img*255.0)\n",
        "\n",
        "            loss = mse_loss\n",
        "\n",
        "            progress_bar.set_description(\n",
        "            \"Epoch : {} Validation Loss : {} \".format(epoch, loss))\n",
        "        except :\n",
        "            continue\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNpkA_8fG2cH"
      },
      "source": [
        "**Test Loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mr6f0cRmBtXi"
      },
      "source": [
        "def test_epoch(test_loader, model, optimizer, epoch):\n",
        "\n",
        "    progress_bar = tqdm(enumerate(test_loader))\n",
        "    total_loss = 0.0\n",
        "\n",
        "    no_img_to_write = 10\n",
        "    inference_folder = \"/content/IMAGE_SUPER_RESOLVE_DATA/inference_data\"\n",
        "    if not os.path.isdir(inference_folder):\n",
        "        os.mkdir(inference_folder)\n",
        "\n",
        "    if not os.path.isdir(os.path.join(inference_folder, str(epoch))):\n",
        "        os.mkdir(os.path.join(inference_folder, str(epoch)))\n",
        "\n",
        "    for step, (high_res_img, low_res_img) in progress_bar:\n",
        "\n",
        "        try:\n",
        "            if high_res_img is None and low_res_img is None:\n",
        "                continue\n",
        "\n",
        "            high_res_img = high_res_img.to(device)\n",
        "            low_res_img = low_res_img.to(device)\n",
        "\n",
        "            mse = nn.MSELoss()\n",
        "            l1 = nn.L1Loss()\n",
        "            psnr = PSNR()\n",
        "\n",
        "            model.eval()\n",
        "            pred_img = model.forward(low_res_img)\n",
        "\n",
        "            #mse_loss = mse(pred_img, high_res_img)\n",
        "            #psnr_loss = psnr(pred_img*255.0, high_res_img*255.0)\n",
        "            l1_loss = l1(pred_img, high_res_img)\n",
        "\n",
        "            loss = l1_loss\n",
        "\n",
        "            progress_bar.set_description(\n",
        "            \"Epoch : {} Test Loss : {} \".format(epoch, loss))\n",
        "\n",
        "            if(step < no_img_to_write):\n",
        "\n",
        "                p_img = pred_img.cpu().numpy().transpose(0, 2, 3, 1) * 255\n",
        "                gt_img = high_res_img.cpu().numpy().transpose(0, 2, 3, 1) * 255\n",
        "                inp_img = low_res_img.cpu().numpy().transpose(0, 2, 3, 1) * 255\n",
        "\n",
        "                cv2.imwrite(os.path.join(inference_folder, str(epoch),\n",
        "                        \"img_\"+str(step)+\"_pred.jpg\"), p_img[0])\n",
        "                cv2.imwrite(os.path.join(inference_folder, str(epoch),\n",
        "                        \"img_\"+str(step)+\"_gt.jpg\"), gt_img[0])\n",
        "                cv2.imwrite(os.path.join(inference_folder, str(epoch),\n",
        "                        \"img_\"+str(step)+\"_inp.jpg\"), inp_img[0])\n",
        "        except :\n",
        "            continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1t3jslWRG6sR"
      },
      "source": [
        "**Code to **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOj06fhkBfON"
      },
      "source": [
        "def train_val_test(train_loader, val_loader, test_loader, model, optimizer, n_epoch, resume):\n",
        "\n",
        "    checkpoint_path = \"/content/IMAGE_SUPER_RESOLVE_DATA/checkpoint.pt\"\n",
        "\n",
        "    epoch = 0\n",
        "    if resume:\n",
        "        model, optimizer, epoch = load_ckp(\n",
        "            checkpoint_path, model, optimizer)\n",
        "\n",
        "    while 1:\n",
        "        model, optimizer = train_epoch(train_loader, model, optimizer, epoch)\n",
        "        checkpoint = {'epoch': epoch+1, 'state_dict': model.state_dict(),\n",
        "                      'optimizer': optimizer.state_dict()}\n",
        "        save_ckp(checkpoint, checkpoint_path)\n",
        "        print(\"Checkpoint Saved\")\n",
        "        model, optimizer, epoch = load_ckp(checkpoint_path, model, optimizer)\n",
        "        print(\"Checkpoint Loaded\")\n",
        "        with torch.no_grad():\n",
        "            val_epoch(val_loader, model, optimizer, epoch)\n",
        "            test_epoch(test_loader, model, optimizer, epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d80vuRX7nDFM"
      },
      "source": [
        "def main():\n",
        "\n",
        "    parent_folder = \"/content/IMAGE_SUPER_RESOLVE_DATA\"\n",
        "    extracted_data_folder = os.path.join(parent_folder, \"extracted_data\")\n",
        "    image_data_folder = os.path.join(extracted_data_folder, \"images\")\n",
        "    image_address_list = get_image_address(image_data_folder)\n",
        "    random.shuffle(image_address_list)\n",
        "\n",
        "    train_img_addr_list = image_address_list[:int(0.7*len(image_address_list))]\n",
        "    val_img_addr_list = image_address_list[len(train_img_addr_list):int(\n",
        "        len(train_img_addr_list) + 0.2*len(image_address_list))]\n",
        "    test_img_addr_list = image_address_list[len(\n",
        "        train_img_addr_list) + len(val_img_addr_list):]\n",
        "\n",
        "    print(\"Total Number of Images : \", len(image_address_list))\n",
        "    print(\"Train : {} Val : {} Test : {}\".format(\n",
        "        len(train_img_addr_list), len(val_img_addr_list), len(test_img_addr_list)))\n",
        "\n",
        "    train_loader = load_data(\n",
        "        train_img_addr_list, batch_size=2, num_workers=2, shuffle=True)\n",
        "    val_loader = load_data(val_img_addr_list, batch_size=2,\n",
        "                           num_workers=10, shuffle=True)\n",
        "    test_loader = load_data(\n",
        "        test_img_addr_list, batch_size=1, num_workers=10, shuffle=False)\n",
        "\n",
        "    model = Image_Super_Resolve()\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.Adam(\n",
        "        [p for p in model.parameters() if p.requires_grad], lr=0.01)\n",
        "    n_epoch = 100\n",
        "    resume = False\n",
        "    train_val_test(train_loader, val_loader, test_loader,\n",
        "                   model, optimizer, n_epoch, resume)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "R8TmBvZEDTp6",
        "outputId": "52269ea1-8366-4a40-aa97-63a464fdb09c"
      },
      "source": [
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Files :  7390\n",
            "Number of Files after removing :  7384\n",
            "Total Number of Images :  7384\n",
            "Train : 5168 Val : 1476 Test : 740\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch : 0 Training Loss : 0.03344202786684036 : : 2584it [15:52,  2.71it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-41-f10128d20286>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mresume\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     train_val_test(train_loader, val_loader, test_loader,\n\u001b[0;32m---> 33\u001b[0;31m                    model, optimizer, n_epoch, resume)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-40-4e6b869857e3>\u001b[0m in \u001b[0;36mtrain_val_test\u001b[0;34m(train_loader, val_loader, test_loader, model, optimizer, n_epoch, resume)\u001b[0m\n\u001b[1;32m     12\u001b[0m         checkpoint = {'epoch': epoch+1, 'state_dict': model.state_dict(),\n\u001b[1;32m     13\u001b[0m                       'optimizer': optimizer.state_dict()}\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0msave_ckp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Checkpoint Saved\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_ckp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'save_ckp' is not defined"
          ]
        }
      ]
    }
  ]
}